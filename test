import asyncio
import base64
import io
import time
import wave

import numpy as np
import streamlit as st

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import (
    RealtimeVoiceClient,
    RealtimeSessionParams,
    RealtimeResponseParams,
)

# ----------------------------
# Async runner for Streamlit
# ----------------------------
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        new_loop = asyncio.new_event_loop()
        try:
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
    return asyncio.run(coro)


# ----------------------------
# Audio helpers
# ----------------------------
def wav_to_int16_mono_and_sr(wav_bytes: bytes) -> tuple[np.ndarray, int]:
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        ch = wf.getnchannels()
        sw = wf.getsampwidth()
        sr = wf.getframerate()
        frames = wf.readframes(wf.getnframes())

    if sw != 2:
        raise ValueError(f"Expected 16-bit PCM WAV (sampwidth=2). Got {sw}.")

    x = np.frombuffer(frames, dtype=np.int16)
    if ch == 1:
        mono = x
    elif ch == 2:
        mono = x.reshape(-1, 2).mean(axis=1).astype(np.int16)
    else:
        raise ValueError(f"Unsupported channels={ch}. Expected 1 or 2.")

    return mono, sr


def resample_linear_int16(x: np.ndarray, src_sr: int, dst_sr: int) -> np.ndarray:
    if src_sr == dst_sr:
        return x
    x_f = x.astype(np.float32)
    src_n = len(x_f)
    dst_n = int(round(src_n * (dst_sr / src_sr)))
    src_idx = np.arange(src_n, dtype=np.float32)
    dst_idx = np.linspace(0, src_n - 1, dst_n, dtype=np.float32)
    y = np.interp(dst_idx, src_idx, x_f)
    return np.clip(np.round(y), -32768, 32767).astype(np.int16)


def pcm16_to_wav_bytes(pcm16: bytes, sr: int) -> bytes:
    out = io.BytesIO()
    with wave.open(out, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        wf.writeframes(pcm16)
    return out.getvalue()


# ----------------------------
# Low-level event pump with logging
# ----------------------------
async def recv_events_with_logging(conn, seconds=8.0, max_msgs=500):
    """
    Reads raw websocket messages for a limited time and returns:
    - parsed JSON events (if any)
    - raw frame types seen
    - close info if closed
    """
    events = []
    frame_types = []
    start = time.time()
    close_info = None

    ws = conn.ws

    for _ in range(max_msgs):
        remaining = seconds - (time.time() - start)
        if remaining <= 0:
            break

        try:
            msg = await ws.receive(timeout=remaining)
        except asyncio.TimeoutError:
            break

        frame_types.append(str(msg.type))

        # aiohttp message types are enums; stringifies OK
        if msg.type == 257:  # WSMsgType.CLOSED
            close_info = {"closed": True, "code": ws.close_code, "exception": None}
            break
        if msg.type == 258:  # WSMsgType.ERROR
            close_info = {"closed": True, "code": ws.close_code, "exception": str(ws.exception())}
            break

        # TEXT
        if getattr(msg, "data", None) and isinstance(msg.data, str):
            try:
                evt = __import__("json").loads(msg.data)
                events.append(evt)
            except Exception:
                events.append({"_unparsed_text": msg.data[:4000]})

        # BINARY / PING / PONG etc => ignore content, but we logged frame type
        continue

    return events, frame_types, close_info


# ----------------------------
# Main realtime test
# ----------------------------
async def realtime_test_verbose(
    wav_bytes: bytes,
    *,
    voice: str,
    instructions: str,
    target_sr: int = 24000,
):
    cfg = Config()
    rtc = RealtimeVoiceClient(cfg, ssl_verify=True)

    # push-to-talk (turn_detection=None)
    session_params = RealtimeSessionParams(
        voice=voice,
        input_audio_format="pcm16",
        output_audio_format="pcm16",
        turn_detection=None,
        extra_session={
            "input_audio_sampling_rate": target_sr,
            "output_audio_sampling_rate": target_sr,
        },
    )

    # Start with TEXT+audio requested
    response_params = RealtimeResponseParams(
        modalities=["text", "audio"],
        instructions=instructions,
        extra_response={
            # Extra knobs that some backends expect
            "commit": True,
            "cancel_previous": True,
        },
    )

    conn = await rtc.connect(session_params=session_params, response_params=response_params)

    debug = {
        "connect_frame_types": [],
        "connect_event_types": [],
        "connect_close_info": None,
        "after_text_frame_types": [],
        "after_text_event_types": [],
        "after_text_close_info": None,
        "after_audio_frame_types": [],
        "after_audio_event_types": [],
        "after_audio_close_info": None,
        "assistant_text": "",
        "assistant_pcm16": b"",
    }

    try:
        # 1) Immediately read whatever the server sends after connect/session.update
        evts, frames, close_info = await recv_events_with_logging(conn, seconds=3.0)
        debug["connect_frame_types"] = frames
        debug["connect_event_types"] = [e.get("type", "_no_type") for e in evts]
        debug["connect_close_info"] = close_info

        # 2) TEXT-ONLY sanity check (no mic). If this fails, audio won’t work either.
        await conn.send_event({
            "type": "conversation.item.create",
            "item": {
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": "Reply with one short sentence."}]
            }
        })
        await conn.response_create(override=RealtimeResponseParams(
            modalities=["text"],
            instructions="Reply with one short sentence.",
            extra_response={"commit": True, "cancel_previous": True},
        ))

        evts2, frames2, close_info2 = await recv_events_with_logging(conn, seconds=6.0)
        debug["after_text_frame_types"] = frames2
        debug["after_text_event_types"] = [e.get("type", "_no_type") for e in evts2]
        debug["after_text_close_info"] = close_info2

        # Pull any text deltas
        text_parts = []
        for e in evts2:
            t = e.get("type")
            if t in ("response.text.delta", "response.output_text.delta"):
                d = e.get("delta")
                if d:
                    text_parts.append(d)
            if t == "error":
                debug["after_text_error"] = e

        debug["assistant_text"] = "".join(text_parts).strip()

        # 3) Now do MIC AUDIO -> ask for audio output
        mono, sr = wav_to_int16_mono_and_sr(wav_bytes)
        mono_rs = resample_linear_int16(mono, sr, target_sr)
        pcm = mono_rs.tobytes()

        # chunk
        chunk_bytes = int(target_sr * 2 * 0.1)  # 100ms
        for i in range(0, len(pcm), chunk_bytes):
            await conn.input_audio_append(pcm[i:i+chunk_bytes])

        await conn.input_audio_commit()
        await conn.response_create(override=RealtimeResponseParams(
            modalities=["audio", "text"],
            instructions=instructions,
            extra_response={"commit": True, "cancel_previous": True},
        ))

        evts3, frames3, close_info3 = await recv_events_with_logging(conn, seconds=10.0)
        debug["after_audio_frame_types"] = frames3
        debug["after_audio_event_types"] = [e.get("type", "_no_type") for e in evts3]
        debug["after_audio_close_info"] = close_info3

        audio_out = bytearray()
        text_parts2 = []
        for e in evts3:
            t = e.get("type")

            if t in ("response.audio.delta", "response.output_audio.delta"):
                delta = e.get("delta")
                if delta:
                    audio_out.extend(base64.b64decode(delta))

            if t in ("response.text.delta", "response.output_text.delta"):
                d = e.get("delta")
                if d:
                    text_parts2.append(d)

            if t == "error":
                debug["after_audio_error"] = e

        debug["assistant_pcm16"] = bytes(audio_out)
        if text_parts2:
            # keep if mic created extra text response
            debug["assistant_text_from_audio_turn"] = "".join(text_parts2).strip()

        return debug

    finally:
        await conn.close()


# ----------------------------
# Streamlit UI
# ----------------------------
st.set_page_config(page_title="Realtime Voice Verbose", layout="centered")
st.title("Realtime Voice Verbose Debugger")

voice = st.selectbox("Voice", ["alloy", "aria", "sage", "ember", "verse", "coral"], index=0)
instructions = st.text_area("Instructions", "Reply briefly.", height=80)

audio_file = st.audio_input("Record from mic", sample_rate=16000)

if audio_file is not None:
    user_wav = audio_file.read()
    st.audio(user_wav, format="audio/wav")

    if st.button("Send to Realtime (Verbose)"):
        try:
            with st.spinner("Running verbose test..."):
                dbg = run_async(realtime_test_verbose(
                    user_wav,
                    voice=voice,
                    instructions=instructions.strip() or "Reply briefly.",
                    target_sr=24000,
                ))

            st.subheader("1) After connect/session.update")
            st.write({"frame_types": dbg["connect_frame_types"], "event_types": dbg["connect_event_types"], "close": dbg["connect_close_info"]})

            st.subheader("2) Text-only sanity check")
            st.write({"assistant_text": dbg.get("assistant_text"), "frame_types": dbg["after_text_frame_types"], "event_types": dbg["after_text_event_types"], "close": dbg["after_text_close_info"]})
            if "after_text_error" in dbg:
                st.error("Text-only returned an error event")
                st.json(dbg["after_text_error"])

            st.subheader("3) Mic audio turn (audio+text requested)")
            st.write({"pcm16_bytes": len(dbg["assistant_pcm16"]), "frame_types": dbg["after_audio_frame_types"], "event_types": dbg["after_audio_event_types"], "close": dbg["after_audio_close_info"]})
            if "after_audio_error" in dbg:
                st.error("Audio turn returned an error event")
                st.json(dbg["after_audio_error"])

            if len(dbg["assistant_pcm16"]) > 0:
                wav_out = pcm16_to_wav_bytes(dbg["assistant_pcm16"], 24000)
                st.success("Got assistant audio ✅")
                st.audio(wav_out, format="audio/wav")
            else:
                st.error("No assistant audio bytes were received.")

        except Exception as e:
            st.exception(e)
