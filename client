import requests
import base64
import os
from io import BytesIO
import pdfplumber

from reportlab.pdfgen import canvas
from pypdf import PdfWriter, PdfReader

from openai import AzureOpenAI

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import (
    RealtimeVoiceClient,
    RealtimeSessionParams,
    RealtimeResponseParams,
)


# =====================================================
# PDF helpers (unchanged)
# =====================================================

def _has_form_fields(pdf_bytes):
    pdf = PdfReader(BytesIO(pdf_bytes))
    for page in pdf.pages:
        if "/Annots" in page:
            for annotation in page["/Annots"]:
                annot_object = pdf.get_object(annotation)
                if annot_object.get("/Subtype") == "/Widget":
                    return True
    return False


def _flatten_pdf_forms(pdf_file_like):
    input_pdf = PdfReader(pdf_file_like)
    output_pdf = PdfWriter()

    for page in input_pdf.pages:
        packet = BytesIO()
        c = canvas.Canvas(
            packet,
            pagesize=(page.mediabox.upper_right[0], page.mediabox.upper_right[1]),
        )

        if "/Annots" in page:
            for annotation in page["/Annots"]:
                annot_object = input_pdf.get_object(annotation)
                if "/T" in annot_object and "/V" in annot_object:
                    value = annot_object["/V"]
                    if value in ("/Off", None):
                        continue
                    if value == "/Yes":
                        value = "x"

                    rect = annot_object.get("/Rect")
                    if rect:
                        x, y = float(rect[0]), float(rect[1])
                        y += (float(rect[3]) - float(rect[1])) / 2
                        c.setFont("Helvetica", 12)
                        c.drawString(x, y, str(value))

        c.save()
        packet.seek(0)

        overlay_pdf = PdfReader(packet)
        if overlay_pdf.pages:
            page.merge_page(overlay_pdf.pages[0])

        output_pdf.add_page(page)

    output = BytesIO()
    output_pdf.write(output)
    output.seek(0)
    return output.getvalue()


def _convert_pdf_to_base64_images(pdf_bytes):
    base64_images = []

    flattened = _flatten_pdf_forms(BytesIO(pdf_bytes)) if _has_form_fields(pdf_bytes) else pdf_bytes

    with pdfplumber.open(BytesIO(flattened)) as pdf:
        for page in pdf.pages:
            page_image = page.to_image(resolution=200)
            pil_image = page_image.original
            buf = BytesIO()
            pil_image.save(buf, format="PNG")
            base64_images.append(base64.b64encode(buf.getvalue()).decode("utf-8"))

    return base64_images


# =====================================================
# GPT Client
# =====================================================

class GPTClient:
    def __init__(self, config: Config):
        self.config = config

    # -------------------------------------------------
    # Whisper
    # -------------------------------------------------

    def invoke_whisper(self, audio_file=None, audio_path=None):
        cfg = self.config.get_whisper_config()

        if cfg["resource_flag"] != "KEY":
            raise ValueError("Whisper only supported in KEY mode.")

        client = AzureOpenAI(
            azure_endpoint=cfg["api_url"],
            api_key=cfg["api_key"],
            api_version=cfg["api_version"],
        )

        if audio_file:
            return client.audio.transcriptions.create(
                file=audio_file,
                model=cfg["deployment"],
            )

        if audio_path:
            return client.audio.transcriptions.create(
                file=open(audio_path, "rb"),
                model=cfg["deployment"],
            )

        raise ValueError("Must provide audio_file or audio_path.")

    # -------------------------------------------------
    # Embeddings
    # -------------------------------------------------

    def invoke_embedding(self, data):
        cfg = self.config.get_embedding_config()

        if cfg["resource_flag"] == "KEY":
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                api_key=cfg["api_key"],
                api_version=cfg["api_version"],
            )
        else:
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                azure_ad_token_provider=self.config.get_openai_token,
                api_version=cfg["api_version"],
                default_headers={"projectId": cfg["project_id"]},
            )

        result = client.embeddings.create(
            model=cfg["deployment"],
            input=data,
        )

        if isinstance(data, list):
            return [e.embedding for e in result.data]
        return result.data[0].embedding

    # -------------------------------------------------
    # GPT-3.5
    # -------------------------------------------------

    def invoke_gpt_35(self, input_text, temperature=0, top_p=0):
        cfg = self.config.get_gpt_35_config()

        client = AzureOpenAI(
            azure_endpoint=cfg["api_url"],
            api_key=cfg["api_key"],
            api_version=cfg["api_version"],
        )

        resp = client.chat.completions.create(
            model=cfg["deployment"],
            temperature=temperature,
            top_p=top_p,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input_text},
            ],
        )
        return resp.model_dump()

    # -------------------------------------------------
    # GPT-4o (text / image / PDF)
    # -------------------------------------------------

    def invoke_gpt_4o(
        self,
        input_data=None,
        path_to_file=None,
        temperature=0,
        top_p=0,
        response_type=None,
        schema=None,
    ):
        cfg = self.config.get_gpt_4o_config()

        if cfg["resource_flag"] == "KEY":
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                api_key=cfg["api_key"],
                api_version=cfg["api_version"],
            )
        else:
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                azure_ad_token_provider=self.config.get_openai_token,
                api_version=cfg["api_version"],
                default_headers={"projectId": cfg["project_id"]},
            )

        messages = [{"role": "system", "content": "You are a helpful assistant."}]

        if isinstance(input_data, str):
            messages.append({"role": "user", "content": input_data})
        else:
            base64_images = []

            if path_to_file:
                ext = os.path.splitext(path_to_file)[1].lower()
                if ext == ".pdf":
                    base64_images += _convert_pdf_to_base64_images(open(path_to_file, "rb").read())
                else:
                    base64_images.append(
                        base64.b64encode(open(path_to_file, "rb").read()).decode("utf-8")
                    )

            messages.append(
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Analyze the provided document."},
                        *[
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b}"}}
                            for b in base64_images
                        ],
                    ],
                }
            )

        response_format = {"type": "text"}
        if response_type == "json":
            response_format = {"type": "json_object"}
        elif response_type == "structured":
            response_format = schema

        resp = client.beta.chat.completions.parse(
            model=cfg["deployment"],
            temperature=temperature,
            top_p=top_p,
            messages=messages,
            response_format=response_format,
        )
        return resp.model_dump()

    # -------------------------------------------------
    # ðŸ”Š Realtime Voice (NEW)
    # -------------------------------------------------

    async def realtime_voice(
        self,
        *,
        voice: str = "alloy",
        server_vad: bool = True,
        instructions: str | None = None,
    ):
        """
        Open a realtime voice websocket session.
        Uses Config.get_realtime_config() â€” no deployment/version params needed.
        """

        rtc = RealtimeVoiceClient(self.config, ssl_verify=True)

        session_params = RealtimeSessionParams(
            voice=voice,
            turn_detection={"type": "server_vad"} if server_vad else None,
        )

        response_params = RealtimeResponseParams(
            modalities=["text", "audio"],
            instructions=instructions,
        )

        return await rtc.connect(
            session_params=session_params,
            response_params=response_params,
        )
