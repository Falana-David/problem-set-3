# streamlit_realtime_test.py
#
# Minimal Streamlit test for your patched LingoGPTConnector realtime voice.
# - Records from mic (Streamlit audio_input gives WAV)
# - Converts to mono PCM16, resamples to 24k
# - Sends audio via input_audio_buffer.append + commit
# - Calls response.create
# - Collects audio deltas from BOTH:
#     response.audio.delta  AND response.output_audio.delta
# - Stops on BOTH:
#     response.audio.done   AND response.output_audio.done
# - Also prints all event types seen (super important for debugging)

import asyncio
import base64
import io
import json
import wave

import numpy as np
import streamlit as st

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import (
    RealtimeVoiceClient,
    RealtimeSessionParams,
    RealtimeResponseParams,
)


# ----------------------------
# Async runner for Streamlit
# ----------------------------
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        new_loop = asyncio.new_event_loop()
        try:
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()

    return asyncio.run(coro)


# ----------------------------
# Audio helpers
# ----------------------------
def wav_to_int16_mono_and_sr(wav_bytes: bytes) -> tuple[np.ndarray, int]:
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        ch = wf.getnchannels()
        sw = wf.getsampwidth()
        sr = wf.getframerate()
        frames = wf.readframes(wf.getnframes())

    if sw != 2:
        raise ValueError(f"Expected 16-bit PCM WAV (sampwidth=2). Got {sw}.")

    x = np.frombuffer(frames, dtype=np.int16)

    if ch == 1:
        mono = x
    elif ch == 2:
        mono = x.reshape(-1, 2).mean(axis=1).astype(np.int16)
    else:
        raise ValueError(f"Unsupported channels={ch}. Expected 1 or 2.")

    return mono, sr


def resample_linear_int16(x: np.ndarray, src_sr: int, dst_sr: int) -> np.ndarray:
    if src_sr == dst_sr:
        return x

    x_f = x.astype(np.float32)
    src_n = len(x_f)
    dst_n = int(round(src_n * (dst_sr / src_sr)))

    src_idx = np.arange(src_n, dtype=np.float32)
    dst_idx = np.linspace(0, src_n - 1, dst_n, dtype=np.float32)
    y = np.interp(dst_idx, src_idx, x_f)
    return np.clip(np.round(y), -32768, 32767).astype(np.int16)


def pcm16_to_wav_bytes(pcm16: bytes, sr: int) -> bytes:
    out = io.BytesIO()
    with wave.open(out, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        wf.writeframes(pcm16)
    return out.getvalue()


# ----------------------------
# Realtime roundtrip
# ----------------------------
async def realtime_send_mic_and_get_audio(
    wav_bytes: bytes,
    *,
    voice: str = "alloy",
    instructions: str = "Reply briefly.",
    target_sr: int = 24000,
    chunk_ms: int = 100,
    max_events: int = 800,
):
    cfg = Config()
    rtc = RealtimeVoiceClient(cfg, ssl_verify=True)

    # push-to-talk mode: we explicitly commit
    session_params = RealtimeSessionParams(
        voice=voice,
        input_audio_format="pcm16",
        output_audio_format="pcm16",
        turn_detection=None,
        extra_session={
            # being explicit helps some servers
            "input_audio_sampling_rate": target_sr,
            "output_audio_sampling_rate": target_sr,
        },
    )

    response_params = RealtimeResponseParams(
        modalities=["audio", "text"],
        instructions=instructions,
    )

    conn = await rtc.connect(session_params=session_params, response_params=response_params)

    logs = []
    audio_out = bytearray()
    text_out = []

    try:
        # Convert WAV -> mono PCM16 @ target_sr
        mono, sr = wav_to_int16_mono_and_sr(wav_bytes)
        mono_rs = resample_linear_int16(mono, sr, target_sr)
        pcm = mono_rs.tobytes()

        # Stream in chunks
        bytes_per_ms = int(target_sr * 2 / 1000)  # mono PCM16
        chunk_bytes = max(640, bytes_per_ms * chunk_ms)

        for i in range(0, len(pcm), chunk_bytes):
            await conn.input_audio_append(pcm[i : i + chunk_bytes])

        await conn.input_audio_commit()

        # IMPORTANT: uses patched response_create() (includes voice/output_audio_format)
        await conn.response_create()

        saw_audio_done = False
        saw_response_done = False

        # Read events until we see audio done (or we hit max_events)
        count = 0
        async for evt in conn.iter_events():
            count += 1
            t = evt.get("type")
            logs.append(t)

            # AUDIO: support both event name conventions
            if t in ("response.audio.delta", "response.output_audio.delta"):
                delta_b64 = evt.get("delta", "")
                if delta_b64:
                    audio_out.extend(base64.b64decode(delta_b64))

            if t in ("response.audio.done", "response.output_audio.done"):
                saw_audio_done = True
                # sometimes response.done comes after; keep going until both or just break now
                # break is fine because audio is complete
                break

            # TEXT deltas (optional, debugging)
            if t in ("response.text.delta", "response.output_text.delta"):
                d = evt.get("delta")
                if d:
                    text_out.append(d)

            if t == "response.done":
                saw_response_done = True
                # do NOT break here; some servers send audio after response.done

            if t == "error":
                raise RuntimeError(evt)

            if count >= max_events:
                break

        return {
            "assistant_pcm16": bytes(audio_out),
            "assistant_text": "".join(text_out).strip(),
            "event_types": logs,
            "target_sr": target_sr,
            "saw_audio_done": saw_audio_done,
            "saw_response_done": saw_response_done,
        }

    finally:
        await conn.close()


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="Realtime Voice Test", layout="centered")
st.title("Realtime Voice Test (Mic → WS → Audio)")

with st.expander("Settings", expanded=True):
    voice = st.selectbox("Voice", ["alloy", "aria", "sage", "ember", "verse", "coral"], index=0)
    instructions = st.text_area("Instructions", "Reply briefly.", height=80)
    target_sr = st.selectbox("Target sample rate", [24000, 16000], index=0)
    chunk_ms = st.slider("Chunk size (ms)", 20, 200, 100, 10)

st.caption(
    "This uses your Config + patched realtime_voice client. "
    "It listens for both response.audio.delta and response.output_audio.delta."
)

audio_file = st.audio_input("Record from mic", sample_rate=16000)

if audio_file is not None:
    user_wav = audio_file.read()
    st.audio(user_wav, format="audio/wav")

    if st.button("Send to Realtime"):
        try:
            with st.spinner("Connecting + sending mic audio..."):
                result = run_async(
                    realtime_send_mic_and_get_audio(
                        user_wav,
                        voice=voice,
                        instructions=instructions.strip() or "Reply briefly.",
                        target_sr=target_sr,
                        chunk_ms=chunk_ms,
                    )
                )

            st.subheader("What happened")
            st.write(
                {
                    "assistant_pcm16_bytes": len(result["assistant_pcm16"]),
                    "saw_audio_done": result["saw_audio_done"],
                    "saw_response_done": result["saw_response_done"],
                }
            )

            if result["assistant_text"]:
                st.subheader("Assistant text (debug)")
                st.write(result["assistant_text"])

            if len(result["assistant_pcm16"]) == 0:
                st.error("Assistant audio was EMPTY (no audio delta received).")
            else:
                wav_out = pcm16_to_wav_bytes(result["assistant_pcm16"], result["target_sr"])
                st.success("Got assistant audio ✅")
                st.audio(wav_out, format="audio/wav")
                st.download_button(
                    "Download assistant.wav",
                    data=wav_out,
                    file_name="assistant.wav",
                    mime="audio/wav",
                )

            st.subheader("Event types received")
            st.code("\n".join(result["event_types"])[:20000])

        except Exception as e:
            st.exception(e)
