# =====================================================
# LingoGPTConnector/config.py  (patched)
# =====================================================

import os, time
import httpx
from dotenv import load_dotenv

load_dotenv()


class Config:
    def __init__(self):
        self.openai_api_key = os.getenv("LINGO_OPENAI_API_KEY", None)

        if os.getenv("OPENAI_RESOURCE_FLAG") == "KEY":
            self.resource_flag = "KEY"
            self.openai_api_url = os.getenv("LINGO_OPENAI_API_URL_KEY", None)
            self.openai_api_35_version = os.getenv("LINGO_API_35_VERSION_KEY", None)
            self.openai_api_4o_version = os.getenv("LINGO_API_4o_VERSION_KEY", None)
            self.openai_embedding_version = os.getenv("LINGO_EMBEDDING_VERSION_KEY", None)
            self.openai_whisper_version = os.getenv("LINGO_WHISPER_VERSION_KEY", None)

            self.openai_gpt_35_deployment = os.getenv("GPT_35_DEPLOYMENT_KEY", None)
            self.openai_gpt_4o_deployment = os.getenv("GPT_4o_DEPLOYMENT_KEY", None)
            self.openai_embedding_deployment = os.getenv("LINGO_EMBEDDING_DEPLOYMENT_KEY", None)
            self.openai_whisper_deployment = os.getenv("LINGO_WHISPER_DEPLOYMENT_KEY", None)

            # Not used in KEY mode, but keep defined so getters don't crash
            self.openai_project_id = None

        elif os.getenv("OPENAI_RESOURCE_FLAG") == "TOKEN":
            self.resource_flag = "TOKEN"
            self.openai_project_id = os.getenv("LINGO_OPENAI_PROJECT_ID", None)
            self.openai_whisper_version = os.getenv("LINGO_WHISPER_VERSION_TOKEN", None)
            self.openai_api_url = os.getenv("LINGO_OPENAI_API_URL_TOKEN", None)
            self.openai_api_4o_version = os.getenv("LINGO_API_4o_VERSION_TOKEN", None)
            self.openai_embedding_version = os.getenv("LINGO_EMBEDDING_VERSION_TOKEN", None)
            self.openai_api_35_version = os.getenv("LINGO_API_35_VERSION_TOKEN", None)

            self.openai_gpt_35_deployment = os.getenv("GPT_35_DEPLOYMENT_TOKEN", None)
            self.openai_gpt_4o_deployment = os.getenv("GPT_4o_DEPLOYMENT_TOKEN", None)
            self.openai_embedding_deployment = os.getenv("LINGO_EMBEDDING_DEPLOYMENT_TOKEN", None)

            # NOTE: this is exactly whatâ€™s shown in your screenshot (KEY, not TOKEN)
            self.openai_whisper_deployment = os.getenv("LINGO_WHISPER_DEPLOYMENT_KEY", None)

        else:
            raise ValueError(
                "Invalid resource flag. Please provide a resource flag of 'KEY' or 'TOKEN'."
            )

        # -----------------------------
        # Realtime (WebSocket) config
        # -----------------------------
        self.openai_realtime_version = os.getenv("LINGO_REALTIME_API_VERSION", "2025-04-01-preview")
        self.openai_realtime_deployment = os.getenv("LINGO_REALTIME_DEPLOYMENT", None)
        self.openai_realtime_ws_path = os.getenv("LINGO_REALTIME_WS_PATH", "openai/realtime")

    def get_openai_token(self):
        openai_token = os.getenv("OPENAI_TOKEN")
        if openai_token is None:
            return self.generate_new_openai_token()

        openai_token_issue_time = os.getenv("OPENAI_TOKEN_ISSUE_TIME")
        if (openai_token_issue_time is None) or (
            int(time.time()) >= int(openai_token_issue_time) + 3590
        ):
            return self.generate_new_openai_token()

        return openai_token

    def generate_new_openai_token(self):
        auth = os.environ["OPENAI_AUTH_URL"]
        client_id = os.environ["OPENAI_CLIENT_ID"]
        client_secret = os.environ["OPENAI_CLIENT_SECRET"]
        scope = os.environ["OPENAI_SCOPE"]
        grant_type = "client_credentials"
        issue_time = int(time.time())

        with httpx.Client() as client:
            body = {
                "grant_type": grant_type,
                "scope": scope,
                "client_id": client_id,
                "client_secret": client_secret,
            }
            resp = client.post(auth, data=body, timeout=60)
            token = resp.json()["access_token"]

        os.environ["OPENAI_TOKEN"] = token
        os.environ["OPENAI_TOKEN_ISSUE_TIME"] = str(issue_time)
        return token

    def get_gpt_35_config(self):
        return {
            "resource_flag": self.resource_flag,
            "api_key": self.openai_api_key,
            "api_url": self.openai_api_url,
            "api_version": self.openai_api_35_version,
            "deployment": self.openai_gpt_35_deployment,
        }

    def get_gpt_4o_config(self):
        if self.resource_flag == "KEY":
            return {
                "resource_flag": self.resource_flag,
                "api_key": self.openai_api_key,
                "api_url": self.openai_api_url,
                "api_version": self.openai_api_4o_version,
                "deployment": self.openai_gpt_4o_deployment,
            }
        elif self.resource_flag == "TOKEN":
            return {
                "resource_flag": self.resource_flag,
                "api_url": self.openai_api_url,
                "api_version": self.openai_api_4o_version,
                "deployment": self.openai_gpt_4o_deployment,
                "project_id": self.openai_project_id,
            }

    def get_embedding_config(self):
        if self.resource_flag == "KEY":
            return {
                "resource_flag": self.resource_flag,
                "api_key": self.openai_api_key,
                "api_url": self.openai_api_url,
                "api_version": self.openai_embedding_version,
                "deployment": self.openai_embedding_deployment,
            }
        elif self.resource_flag == "TOKEN":
            return {
                "resource_flag": self.resource_flag,
                "api_url": self.openai_api_url,
                "api_version": self.openai_embedding_version,
                "deployment": self.openai_embedding_deployment,
                "project_id": self.openai_project_id,
            }

    def get_whisper_config(self):
        return {
            "resource_flag": self.resource_flag,
            "api_key": self.openai_api_key,
            "api_url": self.openai_api_url,
            "api_version": self.openai_whisper_version,
            "deployment": self.openai_whisper_deployment,
        }

    def get_realtime_config(self):
        if not self.openai_realtime_deployment:
            raise ValueError("Realtime deployment is missing. Set LINGO_REALTIME_DEPLOYMENT in env.")

        base = self.get_gpt_4o_config()

        cfg = {
            "resource_flag": base["resource_flag"],
            "api_url": self.openai_api_url,
            "api_version": self.openai_realtime_version,
            "deployment": self.openai_realtime_deployment,
            "ws_path": self.openai_realtime_ws_path,
        }

        if self.resource_flag == "KEY":
            cfg["api_key"] = self.openai_api_key
        else:
            cfg["project_id"] = self.openai_project_id

        return cfg


# =====================================================
# LingoGPTConnector/realtime_voice.py  (patched)
# =====================================================

from __future__ import annotations

import aiohttp
import base64
import json
import ssl
from dataclasses import dataclass, field
from typing import Any, AsyncIterator, Dict, Optional

from LingoGPTConnector.config import Config


@dataclass
class RealtimeSessionParams:
    voice: str = "alloy"
    input_audio_format: str = "pcm16"
    output_audio_format: str = "pcm16"
    turn_detection: Optional[Dict[str, Any]] = field(default_factory=lambda: {"type": "server_vad"})
    input_audio_transcription: Optional[Dict[str, Any]] = None
    extra_session: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RealtimeResponseParams:
    modalities: list[str] = field(default_factory=lambda: ["text", "audio"])
    instructions: Optional[str] = None
    extra_response: Dict[str, Any] = field(default_factory=dict)


class RealtimeVoiceClient:
    def __init__(self, config: Config, ssl_verify: bool = True):
        self.config = config
        self.ssl_verify = ssl_verify

        if not ssl_verify:
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            self._ssl_context = ctx
        else:
            self._ssl_context = None

    def _to_ws_base_url(self, https_url: str) -> str:
        if https_url.startswith("https://"):
            ws = "wss://" + https_url[len("https://") :]
        elif https_url.startswith("http://"):
            ws = "ws://" + https_url[len("http://") :]
        else:
            ws = https_url
        return ws.rstrip("/") + "/"

    def _build_headers(self, cfg: dict) -> Dict[str, str]:
        headers: Dict[str, str] = {}

        if cfg["resource_flag"] == "KEY":
            api_key = cfg.get("api_key")
            if not api_key:
                raise ValueError("Realtime: api_key missing for resource_flag=KEY")
            headers["api-key"] = api_key
        else:
            token = self.config.get_openai_token()
            headers["Authorization"] = f"Bearer {token}"

            project_id = cfg.get("project_id")
            if project_id:
                headers["projectId"] = str(project_id)

        if cfg.get("x_upstream_env"):
            headers["x-upstream-env"] = str(cfg["x_upstream_env"])

        return headers

    async def connect(
        self,
        *,
        session_params: Optional[RealtimeSessionParams] = None,
        response_params: Optional[RealtimeResponseParams] = None,
    ) -> "RealtimeConnection":
        cfg = self.config.get_realtime_config()

        api_url = cfg.get("api_url")
        if not api_url:
            raise ValueError("Realtime: api_url missing in config.")

        deployment = cfg.get("deployment")
        if not deployment:
            raise ValueError("Realtime: deployment missing in config. Set LINGO_REALTIME_DEPLOYMENT.")

        api_version = cfg.get("api_version")
        if not api_version:
            raise ValueError("Realtime: api_version missing in config.")

        ws_path = (cfg.get("ws_path") or "openai/realtime").lstrip("/")

        ws_base_url = self._to_ws_base_url(api_url)
        headers = self._build_headers(cfg)

        params = {"deployment": deployment, "api-version": api_version}

        http_session = aiohttp.ClientSession(base_url=ws_base_url)
        ws = await http_session.ws_connect(
            ws_path,
            headers=headers,
            params=params,
            ssl=self._ssl_context,
            heartbeat=30,
        )

        conn = RealtimeConnection(
            http_session=http_session,
            ws=ws,
            session_params=session_params or RealtimeSessionParams(),
            response_params=response_params or RealtimeResponseParams(),
        )

        await conn.session_update()
        return conn


class RealtimeConnection:
    def __init__(
        self,
        *,
        http_session: aiohttp.ClientSession,
        ws: aiohttp.ClientWebSocketResponse,
        session_params: RealtimeSessionParams,
        response_params: RealtimeResponseParams,
    ):
        self.http_session = http_session
        self.ws = ws
        self.session_params = session_params
        self.response_params = response_params

    async def close(self) -> None:
        try:
            await self.ws.close()
        finally:
            await self.http_session.close()

    async def send_event(self, event: Dict[str, Any]) -> None:
        await self.ws.send_str(json.dumps(event))

    async def session_update(self) -> None:
        session_obj: Dict[str, Any] = {
            "turn_detection": self.session_params.turn_detection,
            "voice": self.session_params.voice,
            "input_audio_format": self.session_params.input_audio_format,
            "output_audio_format": self.session_params.output_audio_format,
        }

        if self.session_params.input_audio_transcription:
            session_obj["input_audio_transcription"] = self.session_params.input_audio_transcription

        if self.session_params.extra_session:
            session_obj.update(self.session_params.extra_session)

        await self.send_event({"type": "session.update", "session": session_obj})

    async def input_audio_append(self, pcm_bytes: bytes) -> None:
        await self.send_event(
            {
                "type": "input_audio_buffer.append",
                "audio": base64.b64encode(pcm_bytes).decode("utf-8"),
            }
        )

    async def input_audio_commit(self) -> None:
        await self.send_event({"type": "input_audio_buffer.commit"})

    async def response_create(self, *, override: Optional[RealtimeResponseParams] = None) -> None:
        """
        PATCH:
        - include voice + output_audio_format in response.create
        - keep modalities/instructions/extra_response
        """
        rp = override or self.response_params

        resp_obj: Dict[str, Any] = {
            "modalities": rp.modalities,
            # These are often required here for audio output
            "voice": self.session_params.voice,
            "output_audio_format": self.session_params.output_audio_format,
        }

        if rp.instructions:
            resp_obj["instructions"] = rp.instructions

        if rp.extra_response:
            resp_obj.update(rp.extra_response)

        await self.send_event({"type": "response.create", "response": resp_obj})

    async def iter_events(self) -> AsyncIterator[Dict[str, Any]]:
        while True:
            msg = await self.ws.receive()

            if msg.type == aiohttp.WSMsgType.CLOSED:
                break

            if msg.type == aiohttp.WSMsgType.ERROR:
                raise RuntimeError(f"WebSocket error: {msg}")

            if msg.type != aiohttp.WSMsgType.TEXT:
                continue

            yield json.loads(msg.data)

    async def iter_audio_deltas(self) -> AsyncIterator[bytes]:
        """
        PATCH:
        Support BOTH event naming conventions:
        - response.audio.delta / response.audio.done
        - response.output_audio.delta / response.output_audio.done
        """
        async for event in self.iter_events():
            t = event.get("type")

            if t in ("response.audio.delta", "response.output_audio.delta"):
                delta = event.get("delta")
                if delta:
                    yield base64.b64decode(delta)

            elif t in ("response.audio.done", "response.output_audio.done"):
                return

            elif t == "error":
                raise RuntimeError(event)

    async def debug_print_events(self, max_events: int = 200) -> None:
        """
        Optional: call this after response_create() to see what the server emits.
        Prints only types + small lengths (safe).
        """
        i = 0
        async for event in self.iter_events():
            i += 1
            t = event.get("type")
            print("EVENT:", t)

            if t in ("response.audio.delta", "response.output_audio.delta"):
                print("  audio_b64_len:", len(event.get("delta", "")))

            if t in ("response.audio.done", "response.output_audio.done", "response.done"):
                print("  done-ish")

            if t == "error":
                print("  ERROR:", event)
                break

            if i >= max_events:
                print("Reached max_events")
                break


# =====================================================
# LingoGPTConnector/client.py  (patched only where needed)
# (Your GPTClient code below is unchanged except it imports the patched realtime_voice,
#  and the realtime_voice() method works with the patched response_create/audio deltas.)
# =====================================================

import requests
import base64
import os
from io import BytesIO
import pdfplumber

from reportlab.pdfgen import canvas
from pypdf import PdfWriter, PdfReader

from openai import AzureOpenAI

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import (
    RealtimeVoiceClient,
    RealtimeSessionParams,
    RealtimeResponseParams,
)


def _has_form_fields(pdf_bytes):
    pdf = PdfReader(BytesIO(pdf_bytes))
    for page in pdf.pages:
        if "/Annots" in page:
            for annotation in page["/Annots"]:
                annot_object = pdf.get_object(annotation)
                if annot_object.get("/Subtype") == "/Widget":
                    return True
    return False


def _flatten_pdf_forms(pdf_file_like):
    input_pdf = PdfReader(pdf_file_like)
    output_pdf = PdfWriter()

    for page in input_pdf.pages:
        packet = BytesIO()
        c = canvas.Canvas(
            packet,
            pagesize=(page.mediabox.upper_right[0], page.mediabox.upper_right[1]),
        )

        if "/Annots" in page:
            for annotation in page["/Annots"]:
                annot_object = input_pdf.get_object(annotation)
                if "/T" in annot_object and "/V" in annot_object:
                    value = annot_object["/V"]
                    if value in ("/Off", None):
                        continue
                    if value == "/Yes":
                        value = "x"

                    rect = annot_object.get("/Rect")
                    if rect:
                        x, y = float(rect[0]), float(rect[1])
                        y += (float(rect[3]) - float(rect[1])) / 2
                        c.setFont("Helvetica", 12)
                        c.drawString(x, y, str(value))

        c.save()
        packet.seek(0)

        overlay_pdf = PdfReader(packet)
        if overlay_pdf.pages:
            page.merge_page(overlay_pdf.pages[0])

        output_pdf.add_page(page)

    output = BytesIO()
    output_pdf.write(output)
    output.seek(0)
    return output.getvalue()


def _convert_pdf_to_base64_images(pdf_bytes):
    base64_images = []
    flattened = _flatten_pdf_forms(BytesIO(pdf_bytes)) if _has_form_fields(pdf_bytes) else pdf_bytes

    with pdfplumber.open(BytesIO(flattened)) as pdf:
        for page in pdf.pages:
            page_image = page.to_image(resolution=200)
            pil_image = page_image.original
            buf = BytesIO()
            pil_image.save(buf, format="PNG")
            base64_images.append(base64.b64encode(buf.getvalue()).decode("utf-8"))

    return base64_images


class GPTClient:
    def __init__(self, config: Config):
        self.config = config

    def invoke_whisper(self, audio_file=None, audio_path=None):
        cfg = self.config.get_whisper_config()

        if cfg["resource_flag"] != "KEY":
            raise ValueError("Whisper only supported in KEY mode.")

        client = AzureOpenAI(
            azure_endpoint=cfg["api_url"],
            api_key=cfg["api_key"],
            api_version=cfg["api_version"],
        )

        if audio_file:
            return client.audio.transcriptions.create(file=audio_file, model=cfg["deployment"])
        if audio_path:
            return client.audio.transcriptions.create(
                file=open(audio_path, "rb"),
                model=cfg["deployment"],
            )
        raise ValueError("Must provide audio_file or audio_path.")

    def invoke_embedding(self, data):
        cfg = self.config.get_embedding_config()

        if cfg["resource_flag"] == "KEY":
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                api_key=cfg["api_key"],
                api_version=cfg["api_version"],
            )
        else:
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                azure_ad_token_provider=self.config.get_openai_token,
                api_version=cfg["api_version"],
                default_headers={"projectId": cfg["project_id"]},
            )

        result = client.embeddings.create(model=cfg["deployment"], input=data)
        if isinstance(data, list):
            return [e.embedding for e in result.data]
        return result.data[0].embedding

    def invoke_gpt_35(self, input_text, temperature=0, top_p=0):
        cfg = self.config.get_gpt_35_config()

        client = AzureOpenAI(
            azure_endpoint=cfg["api_url"],
            api_key=cfg["api_key"],
            api_version=cfg["api_version"],
        )

        resp = client.chat.completions.create(
            model=cfg["deployment"],
            temperature=temperature,
            top_p=top_p,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input_text},
            ],
        )
        return resp.model_dump()

    def invoke_gpt_4o(
        self,
        input_data=None,
        path_to_file=None,
        temperature=0,
        top_p=0,
        response_type=None,
        schema=None,
    ):
        cfg = self.config.get_gpt_4o_config()

        if cfg["resource_flag"] == "KEY":
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                api_key=cfg["api_key"],
                api_version=cfg["api_version"],
            )
        else:
            client = AzureOpenAI(
                azure_endpoint=cfg["api_url"],
                azure_ad_token_provider=self.config.get_openai_token,
                api_version=cfg["api_version"],
                default_headers={"projectId": cfg["project_id"]},
            )

        messages = [{"role": "system", "content": "You are a helpful assistant."}]

        if isinstance(input_data, str):
            messages.append({"role": "user", "content": input_data})
        else:
            base64_images = []
            if path_to_file:
                ext = os.path.splitext(path_to_file)[1].lower()
                if ext == ".pdf":
                    base64_images += _convert_pdf_to_base64_images(open(path_to_file, "rb").read())
                else:
                    base64_images.append(
                        base64.b64encode(open(path_to_file, "rb").read()).decode("utf-8")
                    )

            messages.append(
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Analyze the provided document."},
                        *[
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b}"}}
                            for b in base64_images
                        ],
                    ],
                }
            )

        response_format = {"type": "text"}
        if response_type == "json":
            response_format = {"type": "json_object"}
        elif response_type == "structured":
            response_format = schema

        resp = client.beta.chat.completions.parse(
            model=cfg["deployment"],
            temperature=temperature,
            top_p=top_p,
            messages=messages,
            response_format=response_format,
        )
        return resp.model_dump()

    async def realtime_voice(
        self,
        *,
        voice: str = "alloy",
        server_vad: bool = True,
        instructions: str | None = None,
    ):
        rtc = RealtimeVoiceClient(self.config, ssl_verify=True)

        session_params = RealtimeSessionParams(
            voice=voice,
            turn_detection={"type": "server_vad"} if server_vad else None,
        )

        response_params = RealtimeResponseParams(
            modalities=["text", "audio"],
            instructions=instructions,
        )

        return await rtc.connect(session_params=session_params, response_params=response_params)
