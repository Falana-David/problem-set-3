# streamlit_app_debug.py
#
# Streamlit app (DEBUG VERSION):
# - Records mic audio in browser (st.audio_input)
# - Resamples to 24kHz mono PCM16
# - Sends to Realtime WS (push-to-talk)
# - LOGS every event type + key payload fields so we can see why audio deltas are missing
# - Builds assistant WAV ONLY if response.audio.delta arrives

import asyncio
import base64
import io
import json
import wave
from datetime import datetime

import numpy as np
import streamlit as st

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import (
    RealtimeVoiceClient,
    RealtimeSessionParams,
    RealtimeResponseParams,
)


# ----------------------------
# Async runner for Streamlit
# ----------------------------
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        new_loop = asyncio.new_event_loop()
        try:
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
    return asyncio.run(coro)


# ----------------------------
# Audio helpers
# ----------------------------
def wav_bytes_to_pcm16_mono(wav_bytes: bytes) -> tuple[np.ndarray, int]:
    """Return (mono_samples_int16, sample_rate) from 16-bit PCM WAV."""
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        channels = wf.getnchannels()
        sampwidth = wf.getsampwidth()
        sample_rate = wf.getframerate()
        frames = wf.readframes(wf.getnframes())

    if sampwidth != 2:
        raise ValueError(f"Expected 16-bit PCM WAV. Got sampwidth={sampwidth}.")

    samples = np.frombuffer(frames, dtype=np.int16)

    if channels == 1:
        mono = samples
    elif channels == 2:
        mono = samples.reshape(-1, 2).mean(axis=1).astype(np.int16)
    else:
        raise ValueError(f"Unsupported channels={channels} (expected 1 or 2).")

    return mono, sample_rate


def resample_linear_int16(x: np.ndarray, src_sr: int, dst_sr: int) -> np.ndarray:
    """Simple linear resample (good enough for voice debugging)."""
    if src_sr == dst_sr:
        return x

    x_f = x.astype(np.float32)
    src_n = len(x_f)
    dst_n = int(round(src_n * (dst_sr / src_sr)))

    src_idx = np.arange(src_n, dtype=np.float32)
    dst_idx = np.linspace(0, src_n - 1, dst_n, dtype=np.float32)

    y = np.interp(dst_idx, src_idx, x_f)
    return np.clip(np.round(y), -32768, 32767).astype(np.int16)


def pcm16_to_wav_bytes(pcm16: bytes, sample_rate: int, channels: int = 1) -> bytes:
    bio = io.BytesIO()
    with wave.open(bio, "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(pcm16)
    return bio.getvalue()


# ----------------------------
# Debug logging helpers
# ----------------------------
def summarize_event(evt: dict) -> dict:
    """
    Return a *safe* summary of an event:
    - event type
    - common ids
    - lengths of big fields (audio/text deltas) instead of full content
    """
    t = evt.get("type")
    summary = {"type": t}

    # Common IDs
    for k in ("event_id", "response_id", "item_id", "conversation_id", "session_id"):
        if k in evt:
            summary[k] = evt[k]

    # Audio delta sizes (don't print full base64)
    if t == "response.audio.delta" and "delta" in evt:
        summary["audio_b64_len"] = len(evt["delta"])
    if t == "input_audio_buffer.append" and "audio" in evt:
        summary["input_audio_b64_len"] = len(evt["audio"])

    # Text delta sizes (don't print full text)
    if t == "response.text.delta" and "delta" in evt:
        summary["text_len"] = len(evt["delta"])

    # Errors (keep message)
    if t == "error":
        summary["error"] = evt.get("error") or evt

    return summary


# ----------------------------
# Realtime roundtrip with full logging
# ----------------------------
async def realtime_roundtrip_with_logging(
    user_pcm16: bytes,
    *,
    input_sr: int = 24000,
    output_sr: int = 24000,
    voice: str = "alloy",
    instructions: str | None = None,
    chunk_ms: int = 100,
    max_events: int = 200,
):
    cfg = Config()
    rtc = RealtimeVoiceClient(cfg, ssl_verify=True)

    # Push-to-talk: no server VAD, we commit manually
    session_params = RealtimeSessionParams(
        voice=voice,
        input_audio_format="pcm16",
        output_audio_format="pcm16",
        turn_detection=None,
        extra_session={
            "input_audio_sampling_rate": input_sr,
        },
    )

    # FORCE AUDIO ONLY so if audio is possible, it MUST stream audio deltas
    response_params = RealtimeResponseParams(
        modalities=["audio"],
        instructions=instructions,
    )

    conn = await rtc.connect(session_params=session_params, response_params=response_params)

    assistant_audio = bytearray()
    transcript = None

    event_summaries = []
    full_events = []

    bytes_per_ms = int(input_sr * 2 / 1000)  # mono pcm16
    chunk_bytes = max(640, bytes_per_ms * chunk_ms)

    try:
        # --- send user audio ---
        for i in range(0, len(user_pcm16), chunk_bytes):
            await conn.input_audio_append(user_pcm16[i : i + chunk_bytes])

        await conn.input_audio_commit()
        await conn.response_create()

        # --- receive events ---
        count = 0
        async for event in conn.iter_events():
            count += 1
            if count > max_events:
                event_summaries.append({"type": "debug.stop", "reason": "max_events_reached"})
                break

            full_events.append(event)
            event_summaries.append(summarize_event(event))

            et = event.get("type")

            if et == "response.audio.delta":
                assistant_audio.extend(base64.b64decode(event["delta"]))

            elif et == "response.done":
                # transcript shape varies; try best-effort
                try:
                    transcript = event["response"]["output"][0]["content"][0].get("transcript")
                except Exception:
                    transcript = None
                break

            elif et == "error":
                raise RuntimeError(event)

    finally:
        await conn.close()

    assistant_wav = pcm16_to_wav_bytes(bytes(assistant_audio), sample_rate=output_sr, channels=1)
    return assistant_wav, transcript, event_summaries, full_events


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="Realtime Voice Debug", layout="centered")
st.title("Realtime Voice Debug (Mic â†’ WAV reply)")

st.warning(
    "This debug app logs realtime websocket events. "
    "Do NOT paste tokens/secrets when sharing logs."
)

with st.expander("Settings", expanded=True):
    voice = st.selectbox("Voice", ["alloy", "verse", "aria", "sage", "ember", "coral"], index=0)
    instructions = st.text_area("Instructions", value="Answer clearly and briefly.", height=80)
    input_sr = st.selectbox("Send audio sample rate", [24000, 16000], index=0)
    output_sr = st.selectbox("Wrap assistant WAV sample rate", [24000, 16000, 48000], index=0)
    max_events = st.slider("Max events to capture", 50, 400, 200, 10)

st.info("Record with your mic, click Send, then scroll to see event logs.")

audio_file = st.audio_input("Record your question", sample_rate=16000)

if audio_file is not None:
    user_wav = audio_file.read()
    st.audio(user_wav, format="audio/wav")

    if st.button("Send (debug)"):
        try:
            user_mono, user_sr = wav_bytes_to_pcm16_mono(user_wav)

            # Resample to chosen input_sr
            user_resampled = resample_linear_int16(user_mono, user_sr, input_sr).astype(np.int16)
            user_pcm_bytes = user_resampled.tobytes()

            with st.spinner("Streaming to model..."):
                assistant_wav, transcript, event_summaries, full_events = run_async(
                    realtime_roundtrip_with_logging(
                        user_pcm_bytes,
                        input_sr=input_sr,
                        output_sr=output_sr,
                        voice=voice,
                        instructions=instructions.strip() or None,
                        max_events=max_events,
                    )
                )

            # Report audio length
            audio_frames = max(0, len(assistant_wav) - 44)
            st.write(f"Assistant WAV bytes: {len(assistant_wav)} (audio payload bytes: {audio_frames})")

            if len(assistant_wav) <= 44:
                st.error("Assistant audio was EMPTY (no response.audio.delta received).")
            else:
                st.success("Got an audio reply!")
                st.audio(assistant_wav, format="audio/wav")

                ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    "Download WAV",
                    data=assistant_wav,
                    file_name=f"assistant_reply_{ts}.wav",
                    mime="audio/wav",
                )

            if transcript:
                st.subheader("Transcript (best effort)")
                st.write(transcript)

            st.subheader("Event summaries (safe)")
            st.code(json.dumps(event_summaries, indent=2)[:20000])

            st.subheader("Full events (truncated)")
            # Full events may include large base64. We truncate display.
            st.code(json.dumps(full_events, indent=2)[:20000])

            st.info(
                "If you share logs, share ONLY the 'Event summaries (safe)' section. "
                "It contains types and lengths, not raw audio."
            )

        except Exception as e:
            st.error(f"Error: {e}")
