import asyncio
import base64
import io
import json
import wave
from datetime import datetime

import numpy as np
import streamlit as st

from LingoGPTConnector.config import Config
from LingoGPTConnector.realtime_voice import RealtimeVoiceClient, RealtimeSessionParams, RealtimeResponseParams


# ----------------------------
# Async runner for Streamlit
# ----------------------------
def run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        new_loop = asyncio.new_event_loop()
        try:
            return new_loop.run_until_complete(coro)
        finally:
            new_loop.close()
    return asyncio.run(coro)


# ----------------------------
# Audio helpers
# ----------------------------
def wav_bytes_to_pcm16_mono(wav_bytes: bytes) -> tuple[np.ndarray, int]:
    """Return (mono_samples_int16, sample_rate) from 16-bit PCM WAV."""
    with wave.open(io.BytesIO(wav_bytes), "rb") as wf:
        ch = wf.getnchannels()
        sw = wf.getsampwidth()
        sr = wf.getframerate()
        frames = wf.readframes(wf.getnframes())

    if sw != 2:
        raise ValueError(f"Expected 16-bit PCM WAV. Got sampwidth={sw}.")

    samples = np.frombuffer(frames, dtype=np.int16)

    if ch == 1:
        mono = samples
    elif ch == 2:
        mono = samples.reshape(-1, 2).mean(axis=1).astype(np.int16)
    else:
        raise ValueError(f"Unsupported channels={ch} (expected 1 or 2).")

    return mono, sr


def resample_linear_int16(x: np.ndarray, src_sr: int, dst_sr: int) -> np.ndarray:
    """Simple linear resample (good enough for voice)."""
    if src_sr == dst_sr:
        return x

    x_f = x.astype(np.float32)
    src_n = len(x_f)
    dst_n = int(round(src_n * (dst_sr / src_sr)))

    src_idx = np.arange(src_n, dtype=np.float32)
    dst_idx = np.linspace(0, src_n - 1, dst_n, dtype=np.float32)

    y = np.interp(dst_idx, src_idx, x_f)
    return np.clip(np.round(y), -32768, 32767).astype(np.int16)


def pcm16_to_wav_bytes(pcm16: bytes, sample_rate: int, channels: int = 1) -> bytes:
    bio = io.BytesIO()
    with wave.open(bio, "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(pcm16)
    return bio.getvalue()


# ----------------------------
# Realtime roundtrip
# ----------------------------
async def realtime_roundtrip(
    user_pcm16: bytes,
    *,
    input_sr: int = 24000,
    output_sr: int = 24000,
    voice: str = "alloy",
    instructions: str | None = None,
    chunk_ms: int = 100,
    show_debug: bool = False,
):
    cfg = Config()
    rtc = RealtimeVoiceClient(cfg, ssl_verify=True)

    # Tell the session what sample rate we are sending.
    session_params = RealtimeSessionParams(
        voice=voice,
        input_audio_format="pcm16",
        output_audio_format="pcm16",
        turn_detection=None,  # push-to-talk
        extra_session={
            "input_audio_sampling_rate": input_sr,
        },
    )

    response_params = RealtimeResponseParams(
        modalities=["text", "audio"],
        instructions=instructions,
    )

    conn = await rtc.connect(session_params=session_params, response_params=response_params)

    assistant_audio = bytearray()
    transcript = None
    debug_events = []

    bytes_per_ms = int(input_sr * 2 / 1000)  # mono pcm16
    chunk_bytes = max(640, bytes_per_ms * chunk_ms)

    try:
        for i in range(0, len(user_pcm16), chunk_bytes):
            await conn.input_audio_append(user_pcm16[i : i + chunk_bytes])

        await conn.input_audio_commit()
        await conn.response_create()

        async for event in conn.iter_events():
            et = event.get("type")

            if show_debug:
                # keep last ~40 events
                debug_events.append(event)
                if len(debug_events) > 40:
                    debug_events.pop(0)

            if et == "response.audio.delta":
                # audio chunk is base64 in event["delta"]
                assistant_audio.extend(base64.b64decode(event["delta"]))

            elif et == "response.done":
                # transcript might be present depending on server output shape
                try:
                    transcript = event["response"]["output"][0]["content"][0].get("transcript")
                except Exception:
                    transcript = None
                break

            elif et == "error":
                raise RuntimeError(event)

    finally:
        await conn.close()

    # Wrap assistant audio into WAV at output_sr
    assistant_wav = pcm16_to_wav_bytes(bytes(assistant_audio), sample_rate=output_sr, channels=1)
    return assistant_wav, transcript, debug_events


# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="Realtime Voice (Lingo)", layout="centered")
st.title("Realtime Voice (Mic â†’ WAV reply)")

with st.expander("Settings", expanded=True):
    voice = st.selectbox("Voice", ["alloy", "verse", "aria", "sage", "ember", "coral"], index=0)
    instructions = st.text_area("Instructions", value="Answer clearly and briefly.", height=80)
    debug = st.checkbox("Show debug events", value=True)

st.info("Record, then click **Send**. This uses push-to-talk (manual commit).")

audio_file = st.audio_input("Record your question", sample_rate=16000)

if audio_file is not None:
    user_wav = audio_file.read()
    st.audio(user_wav, format="audio/wav")

    if st.button("Send"):
        try:
            user_mono, user_sr = wav_bytes_to_pcm16_mono(user_wav)

            # Resample to 24kHz mono PCM16 (recommended for realtime pcm16 defaults)
            target_sr = 24000
            user_24k = resample_linear_int16(user_mono, user_sr, target_sr)
            user_pcm_bytes = user_24k.tobytes()

            with st.spinner("Streaming to model..."):
                assistant_wav, transcript, debug_events = run_async(
                    realtime_roundtrip(
                        user_pcm_bytes,
                        input_sr=target_sr,
                        output_sr=24000,
                        voice=voice,
                        instructions=instructions.strip() or None,
                        show_debug=debug,
                    )
                )

            # Check for empty audio
            if len(assistant_wav) <= 44:  # WAV header is 44 bytes; <=44 => no frames
                st.error("Got response, but assistant audio was empty (no response.audio.delta).")
                if debug and debug_events:
                    st.subheader("Last events (debug)")
                    st.code(json.dumps(debug_events, indent=2)[:12000])
            else:
                st.success("Got an audio reply!")

                if transcript:
                    st.subheader("Transcript")
                    st.write(transcript)

                st.subheader("Assistant reply")
                st.audio(assistant_wav, format="audio/wav")

                ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    "Download WAV",
                    data=assistant_wav,
                    file_name=f"assistant_reply_{ts}.wav",
                    mime="audio/wav",
                )

                if debug:
                    st.subheader("Last events (debug)")
                    st.code(json.dumps(debug_events, indent=2)[:12000])

        except Exception as e:
            st.error(f"Error: {e}")
